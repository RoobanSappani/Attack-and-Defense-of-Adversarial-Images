# Attack-and-Defense-of-Adversial-Images
I have implemented three types of adversial attacks that can be used on a trained CNN model. To countermeasure these attacks, a defense algorithm is also implemented.

# Attack of Adversial Images

Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake. Theyâ€™re like optical illusions for machines.

I have implemented three types of white box attacks:

1. Fast Gradient Sign Method[1](https://arxiv.org/abs/1412.6572)
2. Iterative Fast Gradient Sign Method[2]
3. Momentum Iterative Fast Gradient Sign Method[3]

# Defense from Adversial Images

To countermeasure the above attacks, a 

# References

[1] https://arxiv.org/abs/1412.6572
[2] https://arxiv.org/abs/1607.02533
[3] https://arxiv.org/abs/1710.06081


